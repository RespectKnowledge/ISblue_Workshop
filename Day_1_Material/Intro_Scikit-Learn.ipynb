{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3dd91cfe",
   "metadata": {},
   "source": [
    "# Introduction of OpenCV for Basic Image Processing\n",
    " \n",
    " \n",
    " ## By **Abdul Qayyum, Abdesslam BENZINOU, Kamal NASREDDINE**\n",
    " \n",
    " \n",
    " ### date: 18-09-2021\n",
    " \n",
    " \n",
    " ![title](OpenCV/logo_enib.png)\n",
    " ![title](OpenCV/logo_lab.png)\n",
    " ![title](OpenCV/isblue.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf961f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6dcd103a",
   "metadata": {},
   "source": [
    "# Introduction of scikit-learn for machine learning classifier\n",
    "## main website https://scikit-learn.org/stable/\n",
    "## installation pip install -U scikit-learn\n",
    "\n",
    "\n",
    "## Introduction of scikit-learn\n",
    "## The purpose of this guide is to illustrate some of the main features \n",
    "## that scikit-learn provides. It assumes a very basic working knowledge \n",
    "## of machine learning practices (model fitting, predicting, cross-validation, etc.). \n",
    "\n",
    "\n",
    "## Scikit-learn is an open source machine learning library that supports \n",
    "## supervised and unsupervised learning. \n",
    "### It also provides various tools:\n",
    "### 1. model fitting, \n",
    "### 2. data preprocessing, \n",
    "### 3. model selection \n",
    "### 4. evaluation, \n",
    "### 5. many other utilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbbc145",
   "metadata": {},
   "source": [
    "# Quick start of scikit-learn\n",
    "# Scikit-learn provides dozens of built-in machine learning algorithms and models, called estimators. Each estimator can be fitted to some data using its fit method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee5045dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(random_state=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here is a simple example where we fit a RandomForestClassifier \n",
    "# to some very basic data:\n",
    "#import random forest classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier=RandomForestClassifier(random_state=0)\n",
    "# define arbitrary dataset\n",
    "X=[[1,2,3],[22,34,45]] #2x3(number of samplesxnumber of features)\n",
    "#define label\n",
    "y=[1,0] # two samples means we have 2 labels, classes of each sample, sample1=1,sample2=class2\n",
    "# fit function is an object and used for training the classifiers\n",
    "classifier.fit(X,y) # parameters of fit functions are feature matrix(X) and label y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335d8abe",
   "metadata": {},
   "source": [
    "## Gneral explanation of estimator or classifier in sklearn\n",
    "### The fit method generally accepts 2 inputs:\n",
    "\n",
    "### 1.The samples matrix (or design matrix) X. \n",
    "### The size of X is typically (n_samples, n_features), \n",
    "### which means that samples are represented as rows and features are represented as columns.\n",
    "\n",
    "### 2.The target values y which are real numbers for regression tasks, \n",
    "### or integers for classification (or any other discrete set of values). \n",
    "\n",
    "### 3.For unsupervized learning tasks, y does not need to be specified. \n",
    "### y is usually 1d array where the i th entry corresponds to the target \n",
    "### of the i th sample (row) of X.\n",
    "\n",
    "### 4.Both X and y are usually expected to be numpy arrays \n",
    "### or equivalent array-like data types, \n",
    "### though some estimators work with other formats such as sparse matrices. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0005528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## testining or validation the model\n",
    "# Once the estimator is fitted, \n",
    "# it can be used for predicting target values of new data. \n",
    "# You donâ€™t need to re-train the estimator:\n",
    "classifier.predict(X)  # predict classes of the training data\n",
    "#### testining or predicting on the new dataset\n",
    "classifier.predict([[4, 5, 6], [14, 15, 16]])  # predict classes of new data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11c9ba0",
   "metadata": {},
   "source": [
    "# Transformation of Dataset in sklearn ##############\n",
    "\n",
    "## https://scikit-learn.org/stable/modules/preprocessing.html#standardization-or-mean-removal-and-variance-scaling\n",
    "\n",
    "## 1. Standardization, or mean removal and variance scaling\n",
    "## Standardization of datasets is a common requirement for many machine learning estimators implemented in scikit-learn; they might behave badly if the individual features do not  more or less look like standard normally distributed data: \n",
    "## Gaussian with zero mean and unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f5726aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "X_train=np.array([[1.,-1.,2.],\n",
    "                  [2.,0.,1.],\n",
    "                  [0.,1.,-1.]])\n",
    "\n",
    "scaler=preprocessing.StandardScaler().fit(X_train)\n",
    "scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c97b34d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -1.22474487,  1.06904497],\n",
       "       [ 1.22474487,  0.        ,  0.26726124],\n",
       "       [-1.22474487,  1.22474487, -1.33630621]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.mean_\n",
    "scaler.scale_\n",
    "\n",
    "X_scaled=scaler.transform(X_train)\n",
    "X_scaled "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78a57029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00000000e+00 0.00000000e+00 1.48029737e-16]\n",
      "[1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "###zero mean and unit stdnard deviation\n",
    "print(X_scaled.mean(axis=0))\n",
    "print(X_scaled.std(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4695a7bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.,  1.],\n",
       "       [ 1., -1.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###### another example\n",
    "# dataset Transformers and pre-processors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X = [[0, 15],[1, 10]]\n",
    "# scale data according to computed scaling values\n",
    "StandardScaler().fit(X).transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d80591a",
   "metadata": {},
   "source": [
    "# 2. Scaling features to a range\n",
    "## An alternative standardization is scaling features to lie between a given minimum and maximum value, often between zero and one, or so that the maximum absolute value of each feature is scaled to unit size. \n",
    "## This can be achieved using MinMaxScaler or MaxAbsScaler, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30c3e79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5        0.         1.        ]\n",
      " [1.         0.5        0.66666667]\n",
      " [0.         1.         0.        ]]\n",
      "[0.5        0.5        0.55555556]\n",
      "0.5185185185185185\n",
      "[0.40824829 0.40824829 0.41573971]\n",
      "0.4115946439054235\n",
      "[[-1.5         0.          1.66666667]]\n"
     ]
    }
   ],
   "source": [
    "min_max_scaler=preprocessing.MinMaxScaler()\n",
    "x_train_min_max=min_max_scaler.fit_transform(X_train)\n",
    "\n",
    "print(x_train_min_max)\n",
    "print(x_train_min_max.mean(axis=0))\n",
    "print(x_train_min_max.mean())\n",
    "print(x_train_min_max.std(axis=0))\n",
    "print(x_train_min_max.std())\n",
    "## similary min max can be applied on unseen test dataset\n",
    "X_test = np.array([[-3., -1.,  4.]])\n",
    "X_test_minmax = min_max_scaler.transform(X_test)\n",
    "print(X_test_minmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69861451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 1., 2.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MaxAbsScaler works in a very similar fashion, \n",
    "# but scales in a way that the training data lies within the range [-1, 1] \n",
    "# by dividing through the largest maximum value in each feature. \n",
    "# It is meant for data that is already centered at zero or sparse data.\n",
    "\n",
    "max_abs_scaler = preprocessing.MaxAbsScaler()\n",
    "X_train_maxabs = max_abs_scaler.fit_transform(X_train)\n",
    "X_train_maxabs\n",
    "\n",
    "X_test = np.array([[ -3., -1.,  4.]])\n",
    "X_test_maxabs = max_abs_scaler.transform(X_test)\n",
    "X_test_maxabs\n",
    "max_abs_scaler.scale_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c268f082",
   "metadata": {},
   "source": [
    "# 3. Normalization\n",
    "## Normalization is the process of scaling individual samples to have unit norm. \n",
    "## This process can be useful if you plan to use a quadratic form such as  the dot-product or any other kernel to quantify the similarity of any pair of samples.\n",
    "\n",
    "## This assumption is the base of the Vector Space Model often used in text classification and clustering contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a586289c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.40824829 -0.40824829  0.81649658]\n",
      " [ 1.          0.          0.        ]\n",
      " [ 0.          0.70710678 -0.70710678]]\n",
      "-0.7071067811865475\n",
      "1.0\n",
      "[[ 0.40824829 -0.40824829  0.81649658]\n",
      " [ 1.          0.          0.        ]\n",
      " [ 0.          0.70710678 -0.70710678]]\n",
      "-0.7071067811865475\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# The function normalize provides a quick and easy way to perform \n",
    "# this operation on a single array-like dataset, either using the l1, l2, \n",
    "# or max norms:\n",
    "X = [[ 1., -1.,  2.],[ 2.,  0.,  0.],[ 0.,  1., -1.]]\n",
    "\n",
    "X_normalized = preprocessing.normalize(X, norm='l2')\n",
    "\n",
    "print(X_normalized)\n",
    "\n",
    "print(X_normalized.min())\n",
    "print(X_normalized.max())\n",
    "\n",
    "X_normalized1 = preprocessing.normalize(X)\n",
    "\n",
    "print(X_normalized1)\n",
    "\n",
    "print(X_normalized1.min())\n",
    "print(X_normalized1.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aba95e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5 -0.5  1. ]\n",
      " [ 1.   0.   0. ]\n",
      " [ 0.   1.  -1. ]]\n",
      "-1.0\n",
      "1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.70710678,  0.70710678,  0.        ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_normalizedmax = preprocessing.normalize(X,'max')\n",
    "\n",
    "print(X_normalizedmax)\n",
    "\n",
    "print(X_normalizedmax.min())\n",
    "print(X_normalizedmax.max())\n",
    "####### another way to define normlization of training and testing\n",
    "X_normalizedmaxf = preprocessing.Normalizer().fit(X)\n",
    "X_normalizedmaxf.transform(X)  # training\n",
    "X_normalizedmaxf.transform([[-1.,  1., 0.]]) #testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d91af51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9736842105263158"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############## standard pipeline for classification in scikit learn #######################\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# define pipline object with data normalization and model\n",
    "pipe=make_pipeline(StandardScaler(),\n",
    "                   LogisticRegression()\n",
    "                   )\n",
    "\n",
    "# load the iris dataset and split it into train and test sets\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=0)\n",
    "### fit function for training\n",
    "pipe.fit(X_train,y_train)\n",
    "# we can now use it like any other estimator for prediction and get accuracy\n",
    "accuracy_score(pipe.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801b366c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
