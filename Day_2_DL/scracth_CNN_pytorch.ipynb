{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "scracth_CNN_pytorch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBIUHSAgX9eh",
        "outputId": "f8d46eea-7a59-4a48-f078-125074a4309f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pitrOYEcYWnI",
        "outputId": "2e0556f4-3668-4d8c-9db2-0b82768d2931"
      },
      "source": [
        "cd /content/drive/MyDrive"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5DqZOB_Y6aI",
        "outputId": "3a2610c8-e191-4272-cd1c-294109a9d618"
      },
      "source": [
        "# The CNN model has some combination of layers such as Convoltional layer, ReLU Layer,\n",
        "# BatchNormalization Layer,Maxpooling Layer,Fully connected layer\n",
        "# we can define layer as \n",
        "import torch.nn as nn\n",
        "c1=nn.Conv2d(in_channels=3,out_channels=16,kernel_size=3,stride=1,padding=0)\n",
        "inp=torch.rand(1,3,224,224) #batchxchannelxheightxwidth\n",
        "out=c1(inp)\n",
        "print(out.shape) # 224-3+1=222,224-3+1=222"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 16, 222, 222])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTmew7-FZ78X",
        "outputId": "1410c7b3-b4a8-41fe-e6d4-3ab3f7554fab"
      },
      "source": [
        "# we can define layer as \n",
        "import torch.nn as nn\n",
        "mp=nn.MaxPool2d(2)\n",
        "inp=torch.rand(1,3,224,224) #batchxchannelxheightxwidth\n",
        "out=mp(inp)\n",
        "print(out.shape) # 224/2,224/2"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 3, 112, 112])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJ6rcf5Mae_k",
        "outputId": "19d7c749-d927-4725-a959-58cf46d389a4"
      },
      "source": [
        "# Flatten Layer\n",
        "import torch.nn as nn\n",
        "flatten=nn.Flatten()\n",
        "inp=torch.rand(1,3,224,224) #batchxchannelxheightxwidth\n",
        "out=flatten(inp)\n",
        "print(out.shape) # 3x224x224=150528"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 150528])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTxmMWoAaYXm",
        "outputId": "f4a906e4-792c-4a10-ac2f-c310444b8f39"
      },
      "source": [
        "# Fully connected layer\n",
        "import torch.nn as nn\n",
        "inp=torch.rand(1,3,224,224) #batchxchannelxheightxwidth\n",
        "fm=nn.Flatten()\n",
        "outf=fm(inp)\n",
        "fc=nn.Linear(in_features=3*224*224,out_features=512) # recieve 3x224x224 flatten out andproduced 512\n",
        "fully_connected=fc(outf)\n",
        "print(fully_connected.shape)\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kyhsLujcWm9",
        "outputId": "201bc5be-0343-4db1-8c58-b76523553d77"
      },
      "source": [
        "# reLU layer\n",
        "print(\"before relu\",fully_connected)\n",
        "reluo=nn.ReLU()(fully_connected)\n",
        "print(\"after relu\",reluo)\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "before relu tensor([[ 0.2916, -0.1454,  0.0568, -0.3121,  0.0432,  0.4928,  0.5879,  0.0290,\n",
            "          0.0448,  0.1521, -0.2914,  0.1825, -0.0562, -0.2361,  0.0831,  0.6990,\n",
            "         -0.0950, -0.0108,  0.5735, -0.1586,  0.4508,  0.1373, -0.3740,  0.1405,\n",
            "          0.2955, -0.7502,  0.0943,  0.2133,  0.1420, -0.3495, -0.3032,  0.1604,\n",
            "          0.4806,  0.0869,  0.2132, -0.0372,  0.0611,  0.0596,  0.6900,  0.2656,\n",
            "         -0.3413,  0.4790,  0.5449, -0.0798, -0.3007, -0.4068, -0.2568,  0.4665,\n",
            "          0.6157,  0.1529, -0.1386, -0.0821, -0.2448, -0.7380,  0.6914,  0.2983,\n",
            "         -0.3503,  0.3761, -0.1393,  0.7289,  0.0313, -0.3059, -0.0194, -0.1360,\n",
            "         -0.1717, -0.0904, -0.0729,  0.4143,  0.4348, -0.2038,  0.1900,  0.5593,\n",
            "          0.1614, -0.8154,  0.0653,  0.0670,  0.1675,  0.1725, -0.2267, -0.2213,\n",
            "         -0.1395,  0.3382, -0.0025,  0.1964,  0.1044,  0.1225,  0.1356, -0.8142,\n",
            "         -0.1776, -0.1981,  0.2282,  0.1254, -0.1311, -0.1286,  0.1146, -0.1282,\n",
            "          0.6726,  0.3681, -0.2112, -0.3549,  0.5060,  0.3939, -0.3694,  0.3724,\n",
            "          0.2748,  0.4393,  0.0843,  0.1208, -0.0184,  0.2762,  0.2149, -0.5689,\n",
            "          0.4573,  0.1516,  0.6797, -0.1783, -0.4354, -0.1533, -0.5000, -0.5623,\n",
            "          0.2170, -0.0053, -0.2648,  0.1305,  0.2799, -0.2004, -0.1641,  0.2074,\n",
            "         -0.0580,  0.0593, -0.4508,  0.3281,  0.7888, -0.5699,  0.0515,  0.1324,\n",
            "         -0.2130, -0.6495,  0.3594, -0.1278,  0.1998, -0.2474,  0.2845, -0.6014,\n",
            "          0.4394,  0.2054,  0.8518,  0.1445,  0.1611, -0.8748,  0.2687, -0.2561,\n",
            "         -0.0061, -0.0525,  0.1806, -0.0581, -0.2344,  0.1399,  0.1562, -0.4776,\n",
            "          0.0635, -0.8379, -0.0435, -0.5416, -0.1694, -0.0481, -0.3788, -0.3122,\n",
            "          0.0946,  0.2114,  0.1899, -0.1770, -0.0138,  0.1727, -0.5833,  0.3411,\n",
            "          0.2159, -0.3436, -0.6826,  0.0187, -0.3563, -0.0513, -0.1663,  0.4211,\n",
            "          0.5779, -0.1859,  0.1563, -0.4719, -0.0206, -0.0883, -0.1322,  0.0300,\n",
            "         -0.1201, -0.1626,  0.2636,  0.1546,  0.0333, -0.0726, -0.2674, -0.0200,\n",
            "         -0.1478, -0.0269,  0.6530, -0.1916, -0.1807,  0.0583,  0.3329,  0.1375,\n",
            "          0.1688,  0.1776, -0.1210, -0.1039,  0.0246, -0.5295, -0.0392,  0.4115,\n",
            "         -0.4227,  0.4439,  0.2747,  0.0439, -0.1801,  0.3086,  0.4421,  0.3289,\n",
            "          0.0867, -0.6173, -0.4285,  0.1887,  0.4141, -0.6848, -0.3767,  0.3103,\n",
            "         -0.2225,  0.1527,  0.2110,  0.9316, -0.3272, -0.2900,  0.1161,  0.2573,\n",
            "         -0.5411, -0.2859,  0.4406,  0.1323,  0.5840, -0.0498,  0.4794,  0.3912,\n",
            "          0.1956,  0.1241,  0.1174,  0.1398,  0.0855, -0.0858,  0.3809, -0.2822,\n",
            "         -0.3678,  0.1226, -0.1288,  0.2771,  0.1488,  0.4279, -0.3119,  0.0230,\n",
            "         -0.1234, -0.0551,  0.4205, -0.0829, -0.1466, -0.5330, -0.2212, -0.3452,\n",
            "          0.2587, -0.3948, -0.1032,  0.5601,  0.0544,  0.7992,  0.0309, -0.1145,\n",
            "          0.1790,  0.3622, -0.4378, -0.1847, -0.3891,  0.3058,  0.2855, -0.3162,\n",
            "          0.2102, -0.0334,  0.0388, -0.0906, -0.3390, -0.5440,  0.5248,  0.4084,\n",
            "          0.3579, -0.5441,  0.4830, -0.2288,  0.8514, -0.1025, -0.1322,  0.2409,\n",
            "         -0.4264,  0.3419,  0.4881, -0.3598,  0.0863, -0.2653, -0.0486, -0.2815,\n",
            "         -0.1194,  0.0439,  0.2886, -0.1085, -0.3638, -0.3224, -0.2438,  0.4931,\n",
            "         -0.1045, -0.1753, -0.4812,  0.2058, -0.2238,  0.4219, -0.5303, -0.1926,\n",
            "          0.0865, -0.2052,  0.1666, -0.5042, -0.3178, -0.0325, -0.2755, -0.4695,\n",
            "          0.1942,  0.3347, -0.2148, -0.4335,  0.5444, -0.2706, -0.1585,  0.0440,\n",
            "         -0.2861,  0.2155, -0.2687,  0.1913, -0.4575, -0.7665,  0.2574,  0.5654,\n",
            "         -0.1564,  0.0095,  0.2830, -0.3405, -0.1761,  0.5943,  0.4637,  0.3009,\n",
            "         -0.1577,  0.5791,  0.4148, -0.0389, -0.0956,  0.1484, -0.5889,  0.1372,\n",
            "         -0.0266,  0.5123,  0.7587, -0.6275,  0.0075,  0.0507, -0.2990,  0.4471,\n",
            "         -0.3218, -0.4408, -0.1136,  0.2397, -0.1187, -0.2320, -0.2749,  0.0057,\n",
            "          0.0755,  0.1462, -0.8428,  0.7412, -0.0606,  0.2135, -0.6920, -0.5978,\n",
            "          0.2570, -0.4883, -0.2025,  0.4008, -0.2482,  0.0089, -0.2264, -0.1720,\n",
            "         -0.6601, -0.2329, -0.2919,  0.1853,  0.3025,  0.0011,  0.5721,  0.2289,\n",
            "          0.2715, -0.0897,  0.1233,  0.3689, -0.5959,  0.6726, -0.0521, -0.3276,\n",
            "          0.2481,  0.2482, -0.5428, -0.1650, -0.2245,  0.6866, -0.0045,  0.1237,\n",
            "         -0.4418,  0.0942,  0.3550, -0.2834, -0.3579,  0.1323,  0.7372,  0.1754,\n",
            "          0.4364,  0.2218,  0.1728, -0.0509, -0.1760, -0.1177, -0.0414,  0.0641,\n",
            "         -0.1496,  0.2530, -0.4194, -0.0458, -0.2239,  0.1916, -0.4362, -0.0294,\n",
            "         -0.0402, -0.6844,  0.0519, -0.1039,  0.3963, -0.7808,  0.4763, -0.1180,\n",
            "          0.2114,  0.5818,  0.2631, -0.2292, -0.6585,  0.3365, -0.0337, -0.1919,\n",
            "         -0.2319, -0.4776,  0.2626,  0.2435, -0.4908, -0.2979,  0.3787,  0.5999,\n",
            "         -0.5259,  0.2697,  0.6323, -0.1545,  0.1145,  0.0025, -0.0260, -0.5393,\n",
            "          0.2666, -0.1634,  0.2925, -0.1393,  0.3848, -0.0975, -0.6538,  0.0250,\n",
            "          0.2032, -0.0596, -0.1145, -0.0086,  0.0658,  0.4307, -0.1460,  0.1628,\n",
            "         -0.2908, -0.5971,  0.3663, -0.1023, -0.3209, -0.0268,  0.1423,  0.0209,\n",
            "          0.5164,  0.3366,  0.8914,  0.2250, -0.0215, -0.0792, -0.2456,  0.6483]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "after relu tensor([[0.2916, 0.0000, 0.0568, 0.0000, 0.0432, 0.4928, 0.5879, 0.0290, 0.0448,\n",
            "         0.1521, 0.0000, 0.1825, 0.0000, 0.0000, 0.0831, 0.6990, 0.0000, 0.0000,\n",
            "         0.5735, 0.0000, 0.4508, 0.1373, 0.0000, 0.1405, 0.2955, 0.0000, 0.0943,\n",
            "         0.2133, 0.1420, 0.0000, 0.0000, 0.1604, 0.4806, 0.0869, 0.2132, 0.0000,\n",
            "         0.0611, 0.0596, 0.6900, 0.2656, 0.0000, 0.4790, 0.5449, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.4665, 0.6157, 0.1529, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.6914, 0.2983, 0.0000, 0.3761, 0.0000, 0.7289, 0.0313, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.4143, 0.4348, 0.0000, 0.1900, 0.5593,\n",
            "         0.1614, 0.0000, 0.0653, 0.0670, 0.1675, 0.1725, 0.0000, 0.0000, 0.0000,\n",
            "         0.3382, 0.0000, 0.1964, 0.1044, 0.1225, 0.1356, 0.0000, 0.0000, 0.0000,\n",
            "         0.2282, 0.1254, 0.0000, 0.0000, 0.1146, 0.0000, 0.6726, 0.3681, 0.0000,\n",
            "         0.0000, 0.5060, 0.3939, 0.0000, 0.3724, 0.2748, 0.4393, 0.0843, 0.1208,\n",
            "         0.0000, 0.2762, 0.2149, 0.0000, 0.4573, 0.1516, 0.6797, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.2170, 0.0000, 0.0000, 0.1305, 0.2799, 0.0000,\n",
            "         0.0000, 0.2074, 0.0000, 0.0593, 0.0000, 0.3281, 0.7888, 0.0000, 0.0515,\n",
            "         0.1324, 0.0000, 0.0000, 0.3594, 0.0000, 0.1998, 0.0000, 0.2845, 0.0000,\n",
            "         0.4394, 0.2054, 0.8518, 0.1445, 0.1611, 0.0000, 0.2687, 0.0000, 0.0000,\n",
            "         0.0000, 0.1806, 0.0000, 0.0000, 0.1399, 0.1562, 0.0000, 0.0635, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0946, 0.2114, 0.1899,\n",
            "         0.0000, 0.0000, 0.1727, 0.0000, 0.3411, 0.2159, 0.0000, 0.0000, 0.0187,\n",
            "         0.0000, 0.0000, 0.0000, 0.4211, 0.5779, 0.0000, 0.1563, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0300, 0.0000, 0.0000, 0.2636, 0.1546, 0.0333, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.6530, 0.0000, 0.0000, 0.0583, 0.3329,\n",
            "         0.1375, 0.1688, 0.1776, 0.0000, 0.0000, 0.0246, 0.0000, 0.0000, 0.4115,\n",
            "         0.0000, 0.4439, 0.2747, 0.0439, 0.0000, 0.3086, 0.4421, 0.3289, 0.0867,\n",
            "         0.0000, 0.0000, 0.1887, 0.4141, 0.0000, 0.0000, 0.3103, 0.0000, 0.1527,\n",
            "         0.2110, 0.9316, 0.0000, 0.0000, 0.1161, 0.2573, 0.0000, 0.0000, 0.4406,\n",
            "         0.1323, 0.5840, 0.0000, 0.4794, 0.3912, 0.1956, 0.1241, 0.1174, 0.1398,\n",
            "         0.0855, 0.0000, 0.3809, 0.0000, 0.0000, 0.1226, 0.0000, 0.2771, 0.1488,\n",
            "         0.4279, 0.0000, 0.0230, 0.0000, 0.0000, 0.4205, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.2587, 0.0000, 0.0000, 0.5601, 0.0544, 0.7992, 0.0309,\n",
            "         0.0000, 0.1790, 0.3622, 0.0000, 0.0000, 0.0000, 0.3058, 0.2855, 0.0000,\n",
            "         0.2102, 0.0000, 0.0388, 0.0000, 0.0000, 0.0000, 0.5248, 0.4084, 0.3579,\n",
            "         0.0000, 0.4830, 0.0000, 0.8514, 0.0000, 0.0000, 0.2409, 0.0000, 0.3419,\n",
            "         0.4881, 0.0000, 0.0863, 0.0000, 0.0000, 0.0000, 0.0000, 0.0439, 0.2886,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.4931, 0.0000, 0.0000, 0.0000, 0.2058,\n",
            "         0.0000, 0.4219, 0.0000, 0.0000, 0.0865, 0.0000, 0.1666, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.1942, 0.3347, 0.0000, 0.0000, 0.5444, 0.0000,\n",
            "         0.0000, 0.0440, 0.0000, 0.2155, 0.0000, 0.1913, 0.0000, 0.0000, 0.2574,\n",
            "         0.5654, 0.0000, 0.0095, 0.2830, 0.0000, 0.0000, 0.5943, 0.4637, 0.3009,\n",
            "         0.0000, 0.5791, 0.4148, 0.0000, 0.0000, 0.1484, 0.0000, 0.1372, 0.0000,\n",
            "         0.5123, 0.7587, 0.0000, 0.0075, 0.0507, 0.0000, 0.4471, 0.0000, 0.0000,\n",
            "         0.0000, 0.2397, 0.0000, 0.0000, 0.0000, 0.0057, 0.0755, 0.1462, 0.0000,\n",
            "         0.7412, 0.0000, 0.2135, 0.0000, 0.0000, 0.2570, 0.0000, 0.0000, 0.4008,\n",
            "         0.0000, 0.0089, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1853, 0.3025,\n",
            "         0.0011, 0.5721, 0.2289, 0.2715, 0.0000, 0.1233, 0.3689, 0.0000, 0.6726,\n",
            "         0.0000, 0.0000, 0.2481, 0.2482, 0.0000, 0.0000, 0.0000, 0.6866, 0.0000,\n",
            "         0.1237, 0.0000, 0.0942, 0.3550, 0.0000, 0.0000, 0.1323, 0.7372, 0.1754,\n",
            "         0.4364, 0.2218, 0.1728, 0.0000, 0.0000, 0.0000, 0.0000, 0.0641, 0.0000,\n",
            "         0.2530, 0.0000, 0.0000, 0.0000, 0.1916, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0519, 0.0000, 0.3963, 0.0000, 0.4763, 0.0000, 0.2114, 0.5818, 0.2631,\n",
            "         0.0000, 0.0000, 0.3365, 0.0000, 0.0000, 0.0000, 0.0000, 0.2626, 0.2435,\n",
            "         0.0000, 0.0000, 0.3787, 0.5999, 0.0000, 0.2697, 0.6323, 0.0000, 0.1145,\n",
            "         0.0025, 0.0000, 0.0000, 0.2666, 0.0000, 0.2925, 0.0000, 0.3848, 0.0000,\n",
            "         0.0000, 0.0250, 0.2032, 0.0000, 0.0000, 0.0000, 0.0658, 0.4307, 0.0000,\n",
            "         0.1628, 0.0000, 0.0000, 0.3663, 0.0000, 0.0000, 0.0000, 0.1423, 0.0209,\n",
            "         0.5164, 0.3366, 0.8914, 0.2250, 0.0000, 0.0000, 0.0000, 0.6483]],\n",
            "       grad_fn=<ReluBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7G16QiWXYsp2",
        "outputId": "1d1221ee-b7fe-498d-9942-b59644581c25"
      },
      "source": [
        "# scracth based model(define your owen layers)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "######################################### scratch based models #############################\n",
        "class My_model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(My_model,self).__init__()\n",
        "        # layer block 1\n",
        "        self.conv1=nn.Conv2d(in_channels=3,out_channels=10,kernel_size=5) #224-5+1=220\n",
        "        self.relu1=nn.ReLU()\n",
        "        # layer block two\n",
        "        self.batch1=nn.BatchNorm2d(10)\n",
        "        self.maxpool1=nn.MaxPool2d(kernel_size=2) # 220/2=110\n",
        "        self.conv2=nn.Conv2d(in_channels=10,out_channels=20,kernel_size=3) #111-3=108\n",
        "        self.relu2=nn.ReLU()\n",
        "        self.batch2=nn.BatchNorm2d(20)\n",
        "        self.maxpool2=nn.MaxPool2d(kernel_size=2) # 222/2=54\n",
        "        # layer block three\n",
        "        self.conv3=nn.Conv2d(in_channels=20,out_channels=30,kernel_size=5) #55-5=50\n",
        "        self.relu3=nn.ReLU()\n",
        "        self.batch3=nn.BatchNorm2d(30)\n",
        "        self.maxpool3=nn.MaxPool2d(kernel_size=2) # 222/2=50/2=25\n",
        "        self.fc1=nn.Linear(25*25*30,20)\n",
        "        self.fc2=nn.Linear(20,2)\n",
        "        \n",
        "        \n",
        "    def forward(self,x):\n",
        "        x=self.batch1(self.relu1(self.conv1(x)))\n",
        "        x=self.maxpool1(x)\n",
        "        print(x.shape)\n",
        "        x=self.batch2(self.relu2(self.conv2(x)))\n",
        "        x=self.maxpool2(x)\n",
        "        print(x.shape)\n",
        "        x=self.batch3(self.relu3(self.conv3(x)))\n",
        "        x=self.maxpool3(x)\n",
        "        print(x.shape)\n",
        "        x=x.view(-1,25*25*30)\n",
        "        print(x.shape)\n",
        "        x=self.fc1(x)\n",
        "        print(x.shape)\n",
        "        x=self.fc2(x)\n",
        "        #print(x.shape)\n",
        "        return x\n",
        "    \n",
        "#inp=torch.rand(5,3,224,224)\n",
        "model=My_model()\n",
        "print(model)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My_model(\n",
            "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (relu1): ReLU()\n",
            "  (batch1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(10, 20, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (relu2): ReLU()\n",
            "  (batch2): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv3): Conv2d(20, 30, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (relu3): ReLU()\n",
            "  (batch3): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (maxpool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=18750, out_features=20, bias=True)\n",
            "  (fc2): Linear(in_features=20, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ]
    }
  ]
}